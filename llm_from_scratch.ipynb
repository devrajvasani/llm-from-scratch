{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOLUWSM3vick4ukq0ExRu4n",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/devrajvasani/llm-from-scratch/blob/main/llm_from_scratch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LLM From Scratch\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "a1jlnD7XQtP4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Simple Tokenizer"
      ],
      "metadata": {
        "id": "cICA1ydoK3FE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 1: Creating Tokens"
      ],
      "metadata": {
        "id": "_btv1CQKv8xD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"sample_data/the-verdict.txt\", \"r\", encoding=\"utf-8\") as f:\n",
        "  raw_text = f.read()\n",
        "\n",
        "print(\"Total number of character:\", len(raw_text))\n",
        "print(raw_text[:99])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6RVopDB8qxZz",
        "outputId": "6e2fe7b5-19e4-4926-fd98-c691b0b49b82"
      },
      "execution_count": 207,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total number of character: 4029\n",
            "Once-promising attorney Frank Galvin is an alcoholic ambulance chaser. As a favor, his former partn\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "# Split from white spaces\n",
        "result = re.split(r'([,.]|\\s)', raw_text)\n",
        "\n",
        "# Include special characters\n",
        "result = re.split(r'([,.:;?_!\"()\\']|--|\\s)', raw_text)\n",
        "\n",
        "# Remove spaces\n",
        "result = [item for item in result if item.strip()]\n",
        "print(result)\n",
        "\n",
        "preprocessed = result"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ddd74420-d283-4d97-bd01-e82a54a3b7c2",
        "id": "1QAGlPkBtz0r"
      },
      "execution_count": 208,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Once-promising', 'attorney', 'Frank', 'Galvin', 'is', 'an', 'alcoholic', 'ambulance', 'chaser', '.', 'As', 'a', 'favor', ',', 'his', 'former', 'partner', 'Mickey', 'Morrissey', 'sends', 'him', 'a', 'medical', 'malpractice', 'case', 'which', 'is', 'all', 'but', 'certain', 'to', 'be', 'settled', 'for', 'a', 'significant', 'amount', '.', 'The', 'case', 'involves', 'Deborah', 'Ann', 'Kaye', ',', 'who', 'was', 'left', 'comatose', 'after', 'choking', 'on', 'her', 'own', 'vomit', 'when', 'she', 'received', 'general', 'anesthesia', 'during', 'childbirth', 'at', 'a', 'Catholic', 'hospital', '.', 'The', 'plaintiffs', ',', 'Kaye', \"'\", 's', 'sister', 'and', 'brother-in-law', ',', 'intend', 'to', 'use', 'the', 'settlement', 'to', 'pay', 'for', 'her', 'care', '.', 'A', 'Catholic', 'diocese', 'representative', 'offers', 'Galvin', '$210', ',', '000', '(', 'equivalent', 'to', '$576', ',', '000', 'in', '2024[5]', ')', '.', 'Deeply', 'affected', 'by', 'seeing', 'Kaye', ',', 'Galvin', 'declines', 'and', 'states', 'his', 'intention', 'to', 'try', 'the', 'case', ',', 'stunning', 'the', 'defendants', 'and', 'the', 'judge', '.', 'While', 'preparing', 'for', 'trial', ',', 'Galvin', 'encounters', 'divorcÃ©e', 'Laura', 'Fischer', 'in', 'a', 'bar', ',', 'and', 'they', 'become', 'romantically', 'involved', '.', 'Galvin', 'experiences', 'several', 'setbacks', '.', 'His', 'medical', 'expert', 'disappears', ',', 'and', 'a', 'hastily', 'arranged', 'substitute', \"'\", 's', 'credentials', 'are', 'challenged', '.', 'Nobody', 'who', 'was', 'in', 'the', 'delivery', 'room', 'is', 'willing', 'to', 'testify', 'that', 'negligence', 'occurred', '.', 'The', 'hospital', \"'\", 's', 'attorney', ',', 'Ed', 'Concannon', ',', 'has', 'a', 'large', 'legal', 'team', 'that', 'is', 'masterful', 'with', 'the', 'press', '.', 'Kaye', \"'\", 's', 'brother-in-law', 'angrily', 'confronts', 'Galvin', 'after', 'Concannon', \"'\", 's', 'team', 'tells', 'him', 'of', 'the', 'settlement', 'offer', 'that', 'Galvin', 'rejected', '.', 'It', \"'\", 's', 'a', 'really', 'good', 'offer', '.', 'In', 'chambers', ',', 'Judge', 'Hoyle', 'has', 'a', 'heated', 'exchange', 'with', 'Galvin', 'and', 'threatens', 'him', 'with', 'disbarment', '.', 'Galvin', 'dismisses', 'Hoyle', 'as', 'a', '\"', 'Bag', 'Man', '\"', 'for', 'the', 'local', 'political', 'machine', 'and', '\"', 'a', 'defendant', \"'\", 's', 'judge', '\"', 'who', '\"', 'couldn', \"'\", 't', 'hack', 'it', '\"', 'as', 'a', 'lawyer', '.', 'Hoyle', 'denies', 'Galvin', \"'\", 's', 'motion', 'for', 'a', 'mistrial', 'and', 'threatens', 'to', 'have', 'him', 'arrested', '.', 'Galvin', 'storms', 'out', '.', 'Galvin', 'notices', 'that', 'Kaye', \"'\", 's', 'admitting', 'nurse', ',', 'Kaitlin', 'Costello', ',', 'filled', 'out', 'a', 'form', 'which', 'included', 'the', 'question', ',', '\"', 'When', 'did', 'you', 'last', 'eat', '?', '\"', 'Galvin', 'tracks', 'down', 'Costello', 'in', 'New', 'York', 'City', 'and', 'travels', 'there', 'to', 'request', 'her', 'testimony', '.', 'While', 'Laura', 'arranges', 'to', 'meet', 'Galvin', 'in', 'New', 'York', ',', 'Morrissey', 'finds', 'a', 'check', 'from', 'Concannon', 'in', 'her', 'handbag', 'and', 'realizes', 'she', 'is', 'Concannon', \"'\", 's', 'spy', '.', 'Morrissey', 'also', 'travels', 'to', 'New', 'York', 'and', 'informs', 'Galvin', 'of', 'Laura', \"'\", 's', 'betrayal', '.', 'Galvin', 'confronts', 'her', 'in', 'a', 'bar', 'and', 'strikes', 'her', ',', 'knocking', 'her', 'to', 'the', 'floor', '.', 'On', 'the', 'flight', 'back', 'to', 'Boston', ',', 'Morrissey', 'suggests', 'moving', 'for', 'a', 'mistrial', 'due', 'to', 'Concannon', \"'\", 's', 'ethics', 'violation', ',', 'but', 'Galvin', 'decides', 'to', 'continue', 'the', 'trial', '.', 'In', 'her', 'courtroom', 'testimony', ',', 'Costello', 'says', 'she', 'wrote', 'on', 'the', 'admitting', 'form', 'that', 'Kaye', 'ate', 'a', 'full', 'meal', 'one', 'hour', 'before', 'arriving', 'at', 'the', 'hospital', '.', 'On', 'cross-examination', ',', 'an', 'incredulous', 'Concannon', 'asks', 'how', 'she', 'can', 'prove', 'this', '.', 'Costello', 'reveals', 'that', 'her', 'superiors', 'coerced', 'her', 'into', 'changing', 'the', 'form', 'from', '\"', '1', '\"', 'to', '\"', '9', '\"', ',', 'but', 'before', 'doing', 'so', ',', 'she', 'made', 'a', 'photocopy', 'which', 'she', 'brought', 'to', 'court', '.', 'Concannon', 'objects', 'that', 'for', 'legal', 'purposes', ',', 'the', 'original', 'document', 'is', 'presumed', 'to', 'be', 'correct', ';', 'however', ',', 'Hoyle', 'unexpectedly', 'reserves', 'judgment', '.', 'Costello', 'further', 'testifies', 'that', 'the', 'anesthesiologist', 'later', 'confessed', 'he', 'had', 'failed', 'to', 'read', 'her', 'admitting', 'notes', 'and', 'administered', 'general', 'anesthesia', ',', 'which', 'is', 'dangerous', 'for', 'someone', 'who', 'ate', 'only', 'one', 'hour', 'prior', '.', 'When', 'the', 'anesthesiologist', 'realized', 'his', 'error', ',', 'he', 'threatened', 'to', 'end', 'Costello', \"'\", 's', 'career', 'unless', 'she', 'changed', 'the', 'form', '.', 'After', 'Costello', \"'\", 's', 'testimony', ',', 'Concannon', 'again', 'objects', 'on', 'the', 'grounds', 'that', 'the', 'original', 'admitting', 'document', 'has', 'precedence', '.', 'Hoyle', 'agrees', 'and', 'declares', 'Costello', \"'\", 's', 'testimony', 'stricken', 'from', 'the', 'record', '.', 'Afterward', ',', 'a', 'diocese', 'lawyer', 'praises', 'Concannon', \"'\", 's', 'performance', 'to', 'the', 'bishop', ',', 'who', 'asks', '\"', 'Did', 'you', 'believe', 'her', '?', '\"', ',', 'and', 'is', 'met', 'with', 'embarrassed', 'silence', '.', 'Despite', 'feeling', 'his', 'case', 'is', 'hopeless', ',', 'Galvin', 'gives', 'an', 'impassioned', 'closing', 'argument', '.', 'The', 'jury', 'finds', 'in', 'favor', 'of', 'the', 'plaintiffs', ',', 'and', 'the', 'foreman', 'asks', 'whether', 'the', 'jury', 'can', 'award', 'more', 'than', 'what', 'was', 'sought', '.', 'Hoyle', 'resignedly', 'replies', 'they', 'can', '.', 'As', 'Galvin', 'is', 'congratulated', 'outside', 'the', 'courtroom', ',', 'he', 'glimpses', 'Laura', 'watching', 'him', 'from', 'across', 'the', 'atrium', '.', 'That', 'night', ',', 'a', 'drunk', 'Laura', 'drops', 'her', 'whiskey', 'glass', ',', 'drags', 'her', 'telephone', 'towards', 'her', ',', 'and', 'dials', 'Galvin', \"'\", 's', 'office', 'number', '.', 'Galvin', 'is', 'sitting', 'with', 'a', 'cup', 'of', 'coffee', '.', 'He', 'moves', 'to', 'answer', 'the', 'call', 'but', 'changes', 'his', 'mind', 'and', 'lets', 'it', 'ring', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 2: Creating Token IDs"
      ],
      "metadata": {
        "id": "vpgQzY8N9PEv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "all_words = sorted(set(preprocessed))\n",
        "vocab_size = len(all_words)\n",
        "\n",
        "print(vocab_size)"
      ],
      "metadata": {
        "id": "TvpzokmV9Rk4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c6a4ad48-d64d-47c3-dc19-bdd5225aff71"
      },
      "execution_count": 209,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "348\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vocab = {token:integer for integer,token in enumerate(all_words)}"
      ],
      "metadata": {
        "id": "wPuBniOTv5vV"
      },
      "execution_count": 210,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i, item in enumerate(vocab.items()):\n",
        "    print(item)\n",
        "    if i >= 20:\n",
        "        break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DsKArXmDv_q0",
        "outputId": "59b75227-43b8-44e8-f8f0-57983fa4a9a4"
      },
      "execution_count": 211,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('\"', 0)\n",
            "('$210', 1)\n",
            "('$576', 2)\n",
            "(\"'\", 3)\n",
            "('(', 4)\n",
            "(')', 5)\n",
            "(',', 6)\n",
            "('.', 7)\n",
            "('000', 8)\n",
            "('1', 9)\n",
            "('2024[5]', 10)\n",
            "('9', 11)\n",
            "(';', 12)\n",
            "('?', 13)\n",
            "('A', 14)\n",
            "('After', 15)\n",
            "('Afterward', 16)\n",
            "('Ann', 17)\n",
            "('As', 18)\n",
            "('Bag', 19)\n",
            "('Boston', 20)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class SimpleTokenizerV1:\n",
        "    def __init__(self, vocab):\n",
        "        self.str_to_int = vocab\n",
        "        self.int_to_str = {i:s for s,i in vocab.items()}\n",
        "\n",
        "    def encode(self, text):\n",
        "        preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n",
        "\n",
        "        preprocessed = [\n",
        "          item.strip() for item in preprocessed if item.strip()\n",
        "        ]\n",
        "        ids = [self.str_to_int[s] for s in preprocessed]\n",
        "        return ids\n",
        "\n",
        "    def decode(self, ids):\n",
        "        text = \" \".join([self.int_to_str[i] for i in ids])\n",
        "        # Replace spaces before the specified punctuations\n",
        "        text = re.sub(r'\\s+([,.?!\"()\\'])', r'\\1', text)\n",
        "        return text"
      ],
      "metadata": {
        "id": "IhUJwYvBwBsk"
      },
      "execution_count": 212,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = SimpleTokenizerV1(vocab)\n",
        "\n",
        "text = \"The jury finds in favor of the plaintiffs, and the foreman asks whether the jury can award more than what was sought.\"\n",
        "ids = tokenizer.encode(text)\n",
        "print(ids)\n",
        "text = tokenizer.decode(ids)\n",
        "print(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jKyJMqM1yfRq",
        "outputId": "8a8f1295-b824-481a-c5c5-1ded7fd07bb8"
      },
      "execution_count": 213,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[50, 211, 165, 198, 162, 243, 319, 258, 6, 68, 319, 169, 80, 340, 319, 211, 99, 85, 231, 317, 338, 336, 300, 7]\n",
            "The jury finds in favor of the plaintiffs, and the foreman asks whether the jury can award more than what was sought.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 3: ADDING SPECIAL CONTEXT TOKENS\n",
        "\n",
        "Special context tokens are non-natural language symbols that guide LLMs.\n",
        "\n",
        "<|endoftext|> : signals the end of text for generation control.\n",
        "\n",
        "<|unk|> : handles unknown words, preventing errors by providing a fallback ID.\n",
        "\n",
        "They make tokenizers more robust and help models understand text boundaries and out-of-vocabulary terms effectively. This highlights the need to consider large and diverse\n",
        "training sets to extend the vocabulary when working on LLMs."
      ],
      "metadata": {
        "id": "762EYPYr1-IA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "all_tokens = sorted(list(set(preprocessed)))\n",
        "all_tokens.extend([\"<|endoftext|>\", \"<|unk|>\"])\n",
        "\n",
        "vocab = {token:integer for integer,token in enumerate(all_tokens)}\n",
        "print(\"Length of New Vocabulary: \", len(vocab.items()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DEhVX6871_UA",
        "outputId": "eec776fd-c302-424d-b378-0a8e8ae75b59"
      },
      "execution_count": 214,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length of New Vocabulary:  350\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i, item in enumerate(list(vocab.items())[-5:]):\n",
        "    print(item)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CrRQfKBL8YXF",
        "outputId": "e852425d-f701-4cb4-c14d-69816172b91d"
      },
      "execution_count": 215,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('with', 345)\n",
            "('wrote', 346)\n",
            "('you', 347)\n",
            "('<|endoftext|>', 348)\n",
            "('<|unk|>', 349)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class SimpleTokenizerV2:\n",
        "    def __init__(self, vocab):\n",
        "        self.str_to_int = vocab\n",
        "        self.int_to_str = { i:s for s,i in vocab.items()}\n",
        "\n",
        "    def encode(self, text):\n",
        "        preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n",
        "        preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
        "        preprocessed = [\n",
        "            item if item in self.str_to_int\n",
        "            else \"<|unk|>\" for item in preprocessed\n",
        "        ]\n",
        "\n",
        "        ids = [self.str_to_int[s] for s in preprocessed]\n",
        "        return ids\n",
        "\n",
        "    def decode(self, ids):\n",
        "        text = \" \".join([self.int_to_str[i] for i in ids])\n",
        "        # Replace spaces before the specified punctuations\n",
        "        text = re.sub(r'\\s+([,.:;?!\"()\\'])', r'\\1', text)\n",
        "        return text"
      ],
      "metadata": {
        "id": "vQayD7NF8g1Q"
      },
      "execution_count": 216,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = SimpleTokenizerV2(vocab)\n",
        "\n",
        "text1 = \"Judges Panel can award more than what was sought?.\"\n",
        "text2 = \"The jury finds in support of the plaintiffs.\"\n",
        "\n",
        "text = \" <|endoftext|> \".join((text1, text2))\n",
        "\n",
        "print(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "suq10m3s8wo3",
        "outputId": "aa12ab6b-2016-4f4a-bc22-957a2a0bfe3d"
      },
      "execution_count": 217,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Judges Panel can award more than what was sought?. <|endoftext|> The jury finds in support of the plaintiffs.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.encode(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A3OxMwdq80n3",
        "outputId": "bd5c23eb-3721-4234-afcf-604a7f8aae1f"
      },
      "execution_count": 218,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[349,\n",
              " 349,\n",
              " 99,\n",
              " 85,\n",
              " 231,\n",
              " 317,\n",
              " 338,\n",
              " 336,\n",
              " 300,\n",
              " 13,\n",
              " 7,\n",
              " 348,\n",
              " 50,\n",
              " 211,\n",
              " 165,\n",
              " 198,\n",
              " 349,\n",
              " 243,\n",
              " 319,\n",
              " 258,\n",
              " 7]"
            ]
          },
          "metadata": {},
          "execution_count": 218
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.decode(tokenizer.encode(text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "pqkhvdGx8-Tm",
        "outputId": "ca75b2a0-deec-42a1-ee4f-9e247f58e7cf"
      },
      "execution_count": 219,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'<|unk|> <|unk|> can award more than what was sought?. <|endoftext|> The jury finds in <|unk|> of the plaintiffs.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 219
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Byte Pair Encoding (BPE) Tokenizer"
      ],
      "metadata": {
        "id": "QgnVrltbRcQA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! pip3 install tiktoken"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ptSzI8eRRlrw",
        "outputId": "5fb1a0e2-3fbc-44ad-ffde-37a7847de15e"
      },
      "execution_count": 220,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.12/dist-packages (0.12.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.12/dist-packages (from tiktoken) (2025.11.3)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.12/dist-packages (from tiktoken) (2.32.4)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken) (2025.11.12)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import importlib\n",
        "import tiktoken\n",
        "\n",
        "print(\"tiktoken version:\", importlib.metadata.version(\"tiktoken\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xt_V3lmbSOUR",
        "outputId": "fab26e72-baae-4707-dd57-9325f476bff0"
      },
      "execution_count": 221,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tiktoken version: 0.12.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer_gpt2 = tiktoken.get_encoding(\"gpt2\")\n",
        "tokenizer_cl100k_base = tiktoken.get_encoding(\"cl100k_base\")\n"
      ],
      "metadata": {
        "id": "-Qy6xvnsSY6x"
      },
      "execution_count": 222,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "gpt2 : Used by GPT-2 and early GPT-3 models, Vocabulary Size: ~50,257 tokens\n",
        "\n",
        "cl100k_base : Used by GPT-3.5-turbo, GPT-4, Vocabulary Size: ~100,000 tokens"
      ],
      "metadata": {
        "id": "Q6wCR15lYGAe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = ( \"Hello, do you like tea? <|endoftext|> In the sunlit terraces of someunknownPlace.\")"
      ],
      "metadata": {
        "id": "9iEat_XLWdEJ"
      },
      "execution_count": 223,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenizer: gpt2\n",
        "integers_1 = tokenizer_gpt2.encode(text, allowed_special={\"<|endoftext|>\"})\n",
        "print(integers_1)\n",
        "\n",
        "strings_1 = tokenizer_gpt2.decode(integers_1)\n",
        "print(strings_1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_ew9kwxRShzJ",
        "outputId": "40706067-94a3-4da5-85f0-d6d513299ac8"
      },
      "execution_count": 224,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[15496, 11, 466, 345, 588, 8887, 30, 220, 50256, 554, 262, 4252, 18250, 8812, 2114, 286, 617, 34680, 27271, 13]\n",
            "Hello, do you like tea? <|endoftext|> In the sunlit terraces of someunknownPlace.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenizer: cl100k_base\n",
        "integers_2 = tokenizer_cl100k_base.encode(text, allowed_special={\"<|endoftext|>\"})\n",
        "print(integers_2)\n",
        "\n",
        "strings_2 = tokenizer_cl100k_base.decode(integers_2)\n",
        "print(strings_2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PagsHTYrVHvm",
        "outputId": "d49b7253-d449-44dc-8155-c21bd4a977b8"
      },
      "execution_count": 225,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[9906, 11, 656, 499, 1093, 15600, 30, 220, 100257, 763, 279, 7160, 32735, 7317, 2492, 315, 1063, 16476, 17826, 13]\n",
            "Hello, do you like tea? <|endoftext|> In the sunlit terraces of someunknownPlace.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# For Rendom text\n",
        "integers = tokenizer_cl100k_base.encode(\"iAkwirw ier\")\n",
        "print(integers)\n",
        "\n",
        "strings = tokenizer_cl100k_base.decode(integers)\n",
        "print(strings)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0VWNzoexXO_E",
        "outputId": "a6db9904-c22f-4b6d-8f8e-cbd6be012913"
      },
      "execution_count": 226,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[72, 32, 29700, 404, 86, 602, 261]\n",
            "iAkwirw ier\n"
          ]
        }
      ]
    }
  ]
}